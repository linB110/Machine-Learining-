MNIST handwritten number classifier experiment

This experiment is to see what effect can different activation function affect the model accuracy

### DataSet
1. training data 60000 pictures
2. testing data  10000 pictures
3. size is all 28x28

### Model Structure
1. 3 hidden layer
2. epochs is 5/10/30/50/100
3. optimizer is Adam (learning rate is 0.001)

### Explanation for diagrams
1. relu         -> only use relu as activation function
2. sigmoid      -> only use sigmoid as activation function
3. sigmoid_relu -> use relu in first layer and rest is sigmoid as activation function

### Result

